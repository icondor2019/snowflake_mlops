{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import h3\n",
    "import json\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "import matplotlib.pyplot as plt\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.impute import KNNImputer\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "from utils.geospatial_tools import GeoSpatialTools\n",
    "from utils.snowflake_mlops import SnowflakeMLOpsManager\n",
    "\n",
    "# Configuration\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "\n",
    "__geo_tools = GeoSpatialTools()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature Processing & EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load All Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data sources\n",
    "raw_train = pd.read_csv('files/train.csv')\n",
    "raw_test = pd.read_csv('files/test.csv')\n",
    "raw_base = pd.read_csv('files/h8_summary_hours.csv')\n",
    "df_points = pd.read_csv('files/points_of_interest_aggregated.csv')\n",
    "\n",
    "with open('files/point_interest_coordinates.json', 'r') as f:\n",
    "    json_points_coord = json.load(f)\n",
    "\n",
    "# Process base data\n",
    "base = raw_base.drop('Unnamed: 0', axis=1)\n",
    "base = base.groupby('hex_id').agg({\n",
    "    'total_hours': 'sum', 'unique_devices': 'sum', 'unique_days': 'sum',\n",
    "    'work_h1': 'sum', 'work_h2': 'sum', 'work_h3': 'sum', 'work_h4': 'sum',\n",
    "    'wn_h1': 'sum', 'wn_h2': 'sum', 'wn_h3': 'sum', 'wn_h4': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "print(f\"âœ“ Loaded training data: {raw_train.shape}\")\n",
    "print(f\"âœ“ Loaded test data: {raw_test.shape}\")\n",
    "print(f\"âœ“ Loaded base (activity) data: {base.shape}\")\n",
    "print(f\"âœ“ Loaded POI data: {df_points.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temporal_features(df, base_data):\n",
    "    \"\"\"\n",
    "    Create temporal activity pattern features from device data.\n",
    "    \n",
    "    Includes: work/weekend ratios, day/night patterns, activity intensity\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Merge with base activity data\n",
    "    valid_cols = ['hex_id', 'total_hours', 'work_h1', 'work_h2', 'work_h3', 'work_h4',\n",
    "                  'wn_h1', 'wn_h2', 'wn_h3', 'wn_h4']\n",
    "    df = df.merge(base_data[valid_cols], how='left', on='hex_id')\n",
    "    \n",
    "    # Mark missing values (imputation needed)\n",
    "    df['has_activity'] = (~df['total_hours'].isna()).astype(int)\n",
    "    df['missing'] = df['total_hours'].isna().astype(int)\n",
    "    \n",
    "    # Early morning, morning, afternoon, evening, night activity\n",
    "    df['early_morning'] = df['work_h1'] + df['wn_h1']  # 0-5 hours\n",
    "    df['morning'] = df['work_h2'] + df['wn_h2']         # 6-11 hours\n",
    "    df['afternoon'] = df['work_h3'] + df['wn_h3']       # 12-17 hours\n",
    "    df['evening'] = df['work_h4'] + df['wn_h4']         # 18-23 hours\n",
    "    \n",
    "    # Weekday vs weekend patterns\n",
    "    df['weekday_activity'] = df['work_h1'] + df['work_h2'] + df['work_h3'] + df['work_h4']\n",
    "    df['weekend_activity'] = df['wn_h1'] + df['wn_h2'] + df['wn_h3'] + df['wn_h4']\n",
    "    \n",
    "    # Day vs night (night = 18-5, day = 6-17)\n",
    "    df['daytime_activity'] = df['morning'] + df['afternoon']\n",
    "    df['nighttime_activity'] = df['early_morning'] + df['evening']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_distance_features(df, amenities_coord, geo_tools):\n",
    "    \"\"\"\n",
    "    Create distance-to-POI features.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['hex_center'] = df['hex_id'].apply(lambda x: geo_tools.get_h3_center(x))\n",
    "    \n",
    "    # Compute distance features\n",
    "    for poi_type, coor_list in amenities_coord.items():\n",
    "        col_name = f\"dis_to_{poi_type}\"\n",
    "        if coor_list:\n",
    "            start_time = pd.Timestamp.now()\n",
    "            df[col_name] = df['hex_center'].apply(\n",
    "                lambda x, cl=coor_list: geo_tools.hex_distance_from_coordinates_vectorized(hex_center=x, coor_list=cl)\n",
    "            )\n",
    "            end_time = pd.Timestamp.now()\n",
    "        else:\n",
    "            df[col_name] = np.nan\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_poi_features(df, poi_data):\n",
    "    \"\"\"\n",
    "    Create POI density and composition features.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Merge POI data\n",
    "    df = df.merge(poi_data, how='left', on='hex_id')\n",
    "\n",
    "    # Fill missing POI counts with 0\n",
    "    poi_columns = poi_data.columns.difference(['hex_id'])\n",
    "    df['missing_external'] = df['bank'].apply(lambda x: 1 if pd.isna(x) else 0)\n",
    "    df[poi_columns] = df[poi_columns].fillna(0)\n",
    "    \n",
    "    # Total POI count\n",
    "    df['total_poi'] = df[poi_columns].sum(axis=1)\n",
    "    \n",
    "    # POI density (count / activity as proxy for area importance)\n",
    "    df['poi_density'] = df['total_poi'] / (df['total_hours'].fillna(1) + 1)\n",
    "    \n",
    "    # # Positive vs negative POI ratio\n",
    "    # df['positive_poi'] = (df[['health', 'education', 'financial', 'leisure']]\n",
    "    #                       .sum(axis=1))\n",
    "    # df['pos_neg_poi_ratio'] = (df['positive_poi'] + 1) / (df['negative'] + 1)\n",
    "    \n",
    "    # # Essential services\n",
    "    # df['essential_services'] = df['health'] + df['security'] + df['financial']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_geographic_features(df):\n",
    "    \"\"\"\n",
    "    Create geographic/regional features based on H3 hierarchy.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Regions based on H3 resolution 3\n",
    "    costa = ['838f0dfffffffff', '838f28fffffffff', '838f2afffffffff', '838f2bfffffffff', \n",
    "             '838f2cfffffffff', '838f2efffffffff']\n",
    "    sierra = ['8366d1fffffffff', '8366d3fffffffff', '838f29fffffffff', '838f2dfffffffff', \n",
    "              '838f72fffffffff', '838f76fffffffff']\n",
    "    oriente = ['8366dafffffffff']\n",
    "    \n",
    "    df['h_3'] = df['hex_id'].apply(lambda x: h3.cell_to_parent(x, res=3))\n",
    "    df['is_costa'] = df['h_3'].isin(costa).astype(int)\n",
    "    df['is_sierra'] = df['h_3'].isin(sierra).astype(int)\n",
    "    df['is_oriente'] = df['h_3'].isin(oriente).astype(int)\n",
    "    df['missing_points'] = df['bank'].apply(lambda x: 1 if pd.isna(x) else 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def engineer_all_features(df_input, base_data, poi_data, json_coords, geo_tools):\n",
    "    \"\"\"\n",
    "    Apply all feature engineering transformations.\n",
    "    \"\"\"\n",
    "    df = df_input.copy()\n",
    "    \n",
    "    print(\"Creating temporal features...\")\n",
    "    df = create_temporal_features(df, base_data)\n",
    "    \n",
    "    print(\"Creating distance features...\")\n",
    "    df = create_distance_features(df, json_coords, geo_tools)\n",
    "    \n",
    "    print(\"Creating POI features...\")\n",
    "    df = create_poi_features(df, poi_data)\n",
    "    \n",
    "    print(\"Creating geographic features...\")\n",
    "    df = create_geographic_features(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply feature engineering to training data\n",
    "X_raw = engineer_all_features(raw_train, base, df_points, json_points_coord, __geo_tools)\n",
    "print(f\"\\nâœ“ Feature engineering complete. Shape: {X_raw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing & Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(df):\n",
    "    \"\"\"\n",
    "    Handle missing values and create derived features.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Identify rows with missing temporal data\n",
    "    missing_mask = df['missing'] == 1\n",
    "    \n",
    "    # Calculate total hours from hourly components for missing rows\n",
    "    df.loc[missing_mask, 'total_hours'] = (\n",
    "        df.loc[missing_mask, 'work_h1'] + df.loc[missing_mask, 'wn_h1'] +\n",
    "        df.loc[missing_mask, 'work_h2'] + df.loc[missing_mask, 'wn_h2'] +\n",
    "        df.loc[missing_mask, 'work_h3'] + df.loc[missing_mask, 'wn_h3'] +\n",
    "        df.loc[missing_mask, 'work_h4'] + df.loc[missing_mask, 'wn_h4']\n",
    "    )\n",
    "    \n",
    "    # Add small constant to avoid division by zero\n",
    "    df['total_hours'] = df['total_hours'].fillna(0) + 1e-5\n",
    "    \n",
    "    # Activity intensity ratios (normalized by total hours)\n",
    "    df['early_morning_ratio'] = df['early_morning'] / df['total_hours']\n",
    "    df['morning_ratio'] = df['morning'] / df['total_hours']\n",
    "    df['afternoon_ratio'] = df['afternoon'] / df['total_hours']\n",
    "    df['evening_ratio'] = df['evening'] / df['total_hours']\n",
    "    \n",
    "    df['weekday_ratio'] = df['weekday_activity'] / df['total_hours']\n",
    "    df['weekend_ratio'] = df['weekend_activity'] / df['total_hours']\n",
    "    \n",
    "    df['daytime_ratio'] = df['daytime_activity'] / df['total_hours']\n",
    "    df['nighttime_ratio'] = df['nighttime_activity'] / df['total_hours']\n",
    "    \n",
    "    # Peak activity hours\n",
    "    activity_cols = ['early_morning', 'morning', 'afternoon', 'evening']\n",
    "    df['peak_activity'] = df[activity_cols].max(axis=1)\n",
    "    df['activity_variance'] = df[activity_cols].var(axis=1).fillna(0)\n",
    "    \n",
    "    # Distance statistics\n",
    "    distance_columns = [col for col in X_raw.columns if col.startswith('dis_to_')]\n",
    "\n",
    "    df['avg_distance_to_poi'] = df[distance_columns].mean(axis=1)\n",
    "    df['min_distance_to_poi'] = df[distance_columns].min(axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "X = preprocess_features(X_raw)\n",
    "\n",
    "# Handle any remaining NaN values\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "X[numeric_cols] = X[numeric_cols].fillna(X[numeric_cols].median())\n",
    "\n",
    "print(f\"âœ“ Preprocessing complete. Final features: {len(numeric_cols)}\")\n",
    "print(f\"  Missing values: {X[numeric_cols].isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric features for modeling\n",
    "feature_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Remove hex_id from features\n",
    "if 'hex_id' in feature_cols:\n",
    "    feature_cols.remove('hex_id')\n",
    "\n",
    "# Get target\n",
    "y = X['cost_of_living'].copy()\n",
    "X_features = X[feature_cols].copy()\n",
    "\n",
    "# Remove features with zero variance\n",
    "zero_var_features = X_features.columns[X_features.var() < 1e-6].tolist()\n",
    "X_features = X_features.drop(columns=zero_var_features)\n",
    "print(f\"Removed {len(zero_var_features)} zero-variance features\")\n",
    "\n",
    "# Correlation analysis\n",
    "plt.figure(figsize=(14, 8))\n",
    "correlation = X_features.corr()['cost_of_living' if 'cost_of_living' in X_features.columns else y.name].drop('cost_of_living', errors='ignore').sort_values(ascending=False)\n",
    "\n",
    "# Show top 15 features by correlation\n",
    "print(\"\\nðŸ“Š Top 15 Features by Correlation with Target:\")\n",
    "print(correlation.head(15))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "correlation.head(10).plot(kind='barh', ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('Top 10 Positively Correlated Features')\n",
    "axes[0].set_xlabel('Correlation')\n",
    "\n",
    "correlation.tail(5).plot(kind='barh', ax=axes[1], color='coral')\n",
    "axes[1].set_title('Top 5 Negatively Correlated Features')\n",
    "axes[1].set_xlabel('Correlation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select features with correlation > 0.05 (by absolute value)\n",
    "selected_features = correlation[correlation.abs() > 0].index.tolist()\n",
    "print(f\"\\nâœ“ Selected {len(selected_features)} features with |correlation| > 0.05\")\n",
    "\n",
    "# If too few features, use all numeric features\n",
    "if len(selected_features) < 5:\n",
    "    selected_features = feature_cols[:20]  # Use top 20 features\n",
    "    print(f\"  (Using {len(selected_features)} features as fallback)\")\n",
    "\n",
    "X_selected = X_features[selected_features].copy()\n",
    "print(f\"\\nFinal feature set shape: {X_selected.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training & Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Model Diagnostics & Improvements Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_cv(model, X, y, model_name, cv_folds=5):\n",
    "    \"\"\"\n",
    "    Evaluate model with cross-validation.\n",
    "    \"\"\"\n",
    "    kfold = KFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "    # Cross-validation scores (FIX: only negate once for neg_mean_squared_error)\n",
    "    start_time = time.time()\n",
    "    mse_scores = -cross_val_score(model, X, y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "    rmse_scores = np.sqrt(mse_scores)  # Only negate once, not twice!\n",
    "    r2_scores = cross_val_score(model, X, y, cv=kfold, scoring='r2')\n",
    "    mae_scores = -cross_val_score(model, X, y, cv=kfold, scoring='neg_mean_absolute_error')\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'RMSE': rmse_scores.mean(),\n",
    "        'RMSE_std': rmse_scores.std(),\n",
    "        'R2': r2_scores.mean(),\n",
    "        'R2_std': r2_scores.std(),\n",
    "        'MAE': mae_scores.mean(),\n",
    "        'MAE_std': mae_scores.std(),\n",
    "        'Time(s)': elapsed\n",
    "    }\n",
    "    return results, rmse_scores\n",
    "\n",
    "# Initialize models with GPU acceleration where available\n",
    "print(\"\\nðŸ”§ Initializing models...\")\n",
    "USE_GPU = False\n",
    "if USE_GPU:\n",
    "    print(\"  Using GPU acceleration (CUDA) for XGBoost and LightGBM\")\n",
    "else:\n",
    "    print(\"  Using CPU for all models\")\n",
    "\n",
    "models = {\n",
    "    'Random_Forest': RandomForestRegressor(\n",
    "        n_estimators=150,\n",
    "        max_depth=20, \n",
    "        min_samples_split=5, \n",
    "        min_samples_leaf=2,\n",
    "        random_state=RANDOM_STATE, \n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.08,\n",
    "        max_depth=7, \n",
    "        min_samples_split=5,\n",
    "        subsample=0.9,\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "    'XGBoost': xgb.XGBRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.08,\n",
    "        max_depth=7, \n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        min_child_weight=1,\n",
    "        tree_method='hist',\n",
    "        device='cpu',\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "    'LightGBM': lgb.LGBMRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.08,\n",
    "        num_leaves=40,\n",
    "        max_depth=8,\n",
    "        min_child_samples=5,\n",
    "        subsample=0.9,\n",
    "        subsample_freq=1,\n",
    "        device='gpu' if USE_GPU else 'cpu',\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=-1\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸ”„ CROSS-VALIDATION RESULTS (5-Fold)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cv_results = []\n",
    "model_scores = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nðŸ“ Training {name}...\")\n",
    "    results, rmse_scores = evaluate_model_with_cv(model, X_selected, y, name, cv_folds=5)\n",
    "    cv_results.append(results)\n",
    "    model_scores[name] = rmse_scores\n",
    "    \n",
    "    print(f\"   RMSE: {results['RMSE']:.6f} (+/- {results['RMSE_std']:.6f})\")\n",
    "    print(f\"   RÂ²:   {results['R2']:.6f} (+/- {results['R2_std']:.6f})\")\n",
    "    print(f\"   MAE:  {results['MAE']:.6f} (+/- {results['MAE_std']:.6f})\")\n",
    "\n",
    "# Display results\n",
    "cv_df = pd.DataFrame(cv_results).sort_values('RMSE')\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ“Š MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(cv_df.to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "cv_df.plot(x='Model', y='RMSE', kind='bar', ax=axes[0], legend=False, color='steelblue')\n",
    "axes[0].set_title('RMSE by Model')\n",
    "axes[0].set_ylabel('RMSE')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "cv_df.plot(x='Model', y='R2', kind='bar', ax=axes[1], legend=False, color='green')\n",
    "axes[1].set_title('RÂ² by Model')\n",
    "axes[1].set_ylabel('RÂ² Score')\n",
    "\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "# Select best modelprint(f\"   CV RMSE: {cv_df.iloc[0]['RMSE']:.6f}\")\n",
    "\n",
    "\n",
    "best_model_name = cv_df.iloc[0]['Model']\n",
    "print(f\"\\nâœ… Best model: {best_model_name}\")\n",
    "\n",
    "cv_df.plot(x='Model', y='MAE', kind='bar', ax=axes[2], legend=False, color='orange')\n",
    "print(f\"\\nâœ… Best model: {best_model_name}\")\n",
    "best_model_name = cv_df.iloc[0]['Model']\n",
    "\n",
    "axes[2].set_title('MAE by Model')\n",
    "print(f\"   CV RMSE: {cv_df.iloc[0]['RMSE']:.6f}\")# Select best model\n",
    "\n",
    "axes[2].set_ylabel('MAE')\n",
    "\n",
    "axes[2].grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "cv_df.plot(x='Model', y='RMSE', kind='bar', ax=axes[0, 0], legend=False, color='steelblue')\n",
    "axes[0, 0].set_title('RMSE by Model (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('RMSE')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "cv_df.plot(x='Model', y='R2', kind='bar', ax=axes[0, 1], legend=False, color='green')\n",
    "axes[0, 1].set_title('RÂ² by Model (Higher is Better)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('RÂ² Score')\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "cv_df.plot(x='Model', y='MAE', kind='bar', ax=axes[1, 0], legend=False, color='orange')\n",
    "axes[1, 0].set_title('MAE by Model (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('MAE')\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "cv_df.plot(x='Model', y='Time(s)', kind='bar', ax=axes[1, 1], legend=False, color='coral')\n",
    "axes[1, 1].set_title('Training Time (5-Fold CV)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Time (seconds)')\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select best model\n",
    "best_model_name = cv_df.iloc[0]['Model']\n",
    "print(f\"\\nâœ… Best model: {best_model_name}\")\n",
    "print(f\"   CV RMSE: {cv_df.iloc[0]['RMSE']:.6f}\")\n",
    "print(f\"   CV Time: {cv_df.iloc[0]['Time(s)']:.2f}s\")\n",
    "\n",
    "# Show speedup if GPU was used\n",
    "if USE_GPU:\n",
    "    print(f\"\\nðŸš€ GPU acceleration enabled - training should be significantly faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "import os\n",
    "\n",
    "connection_parameters = {\n",
    "    \"account\": os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    \"user\": os.getenv(\"SNOWFLAKE_USER\"),\n",
    "    \"password\": os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
    "    \"role\": os.getenv(\"SNOWFLAKE_ROLE\", \"AI_CORTEX_DEVELOPER_ROLE\"),\n",
    "    \"warehouse\": os.getenv(\"SNOWFLAKE_WAREHOUSE\", \"ai_project_wh\"),\n",
    "    \"database\": os.getenv(\"SNOWFLAKE_DATABASE\", \"ai_project\"),\n",
    "    \"schema\": os.getenv(\"SNOWFLAKE_SCHEMA\", \"mlops\"),\n",
    "}\n",
    "\n",
    "snowflake_session = Session.builder.configs(connection_parameters).create()\n",
    "\n",
    "__snow_mlops = SnowflakeMLOpsManager(snowflake_session, experiment_name=best_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model for tuning (use the best one from CV)\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "# Split data for evaluation (needed for log_run)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_selected, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Define metrics function for experiment tracking\n",
    "def metrics_fn(y_true, y_pred):\n",
    "    return {\n",
    "        \"rmse\": float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
    "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
    "        \"r2_score\": float(r2_score(y_true, y_pred)),\n",
    "    }\n",
    "\n",
    "# Define hyperparameter search space\n",
    "if best_model_name == 'XGBoost':\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 150, 200, 250],\n",
    "        'learning_rate': [0.05, 0.1, 0.15],\n",
    "        'max_depth': [4, 6, 8, 10],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "        'gamma': [0, 0.1, 0.5],\n",
    "    }\n",
    "elif best_model_name == 'LightGBM':\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 150, 200, 250],\n",
    "        'learning_rate': [0.05, 0.1, 0.15],\n",
    "        'num_leaves': [20, 31, 50],\n",
    "        'max_depth': [4, 6, 8],\n",
    "        'min_child_samples': [5, 10, 20],\n",
    "    }\n",
    "elif best_model_name == 'Random_Forest':\n",
    "    param_dist = {\n",
    "        'n_estimators': [150, 200, 300, 500],\n",
    "        'max_depth': [15, 20, 25, 30],\n",
    "        'min_samples_split': [2, 5, 10, 20],\n",
    "        'min_samples_leaf': [1, 2, 4, 8],\n",
    "    }\n",
    "else:  # Gradient Boosting\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 150, 200],\n",
    "        'learning_rate': [0.05, 0.1, 0.15],\n",
    "        'max_depth': [4, 6, 8],\n",
    "        'min_samples_split': [2, 5],\n",
    "    }\n",
    "\n",
    "print(f\"ðŸ” Tuning {best_model_name} with Experiment Tracking...\")\n",
    "print(f\"   Parameter search space: {len(param_dist)} parameters\")\n",
    "\n",
    "# Generate and sample parameter combinations\n",
    "import itertools\n",
    "param_combinations = list(itertools.product(*param_dist.values()))\n",
    "param_names = list(param_dist.keys())\n",
    "n_iter = min(20, len(param_combinations))\n",
    "sampled_indices = np.random.RandomState(RANDOM_STATE).choice(\n",
    "    len(param_combinations), size=n_iter, replace=False\n",
    ")\n",
    "\n",
    "best_metrics = {\"rmse\": float('inf')}\n",
    "best_params = None\n",
    "best_run_name = None\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"ðŸš€ Running {n_iter} hyperparameter experiments with Snowflake MLOps\\n\")\n",
    "\n",
    "# Loop through parameter combinations and log runs\n",
    "for i, idx in enumerate(sampled_indices):\n",
    "    params = dict(zip(param_names, param_combinations[idx]))\n",
    "    run_name = f\"run_{i:02d}_{best_model_name}\"\n",
    "    \n",
    "    print(f\"ðŸ“Œ Run {i+1}/{n_iter}: {run_name} | Params: {params}\")\n",
    "    \n",
    "    # Instantiate fresh model\n",
    "    if best_model_name == 'XGBoost':\n",
    "        model = xgb.XGBRegressor(**params, tree_method='hist', device='cpu', random_state=RANDOM_STATE)\n",
    "    elif best_model_name == 'LightGBM':\n",
    "        model = lgb.LGBMRegressor(**params, device='cpu', random_state=RANDOM_STATE, verbose=-1)\n",
    "    elif best_model_name == 'Random Forest':\n",
    "        model = RandomForestRegressor(**params, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "    else:  # Gradient Boosting\n",
    "        model = GradientBoostingRegressor(**params, random_state=RANDOM_STATE)\n",
    "    \n",
    "    try:\n",
    "        # Log run to Snowflake MLOps\n",
    "        metrics = __snow_mlops.log_run(\n",
    "            run_name=run_name,\n",
    "            model=model,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            params=params,\n",
    "            metrics_fn=metrics_fn,\n",
    "            log_model=False\n",
    "        )\n",
    "        \n",
    "        print(f\"   âœ… RMSE: {metrics['rmse']:.6f} | MAE: {metrics['mae']:.6f} | RÂ²: {metrics['r2_score']:.6f}\\n\")\n",
    "        \n",
    "        # Track best run\n",
    "        if metrics['rmse'] < best_metrics['rmse']:\n",
    "            best_metrics = metrics\n",
    "            best_params = params\n",
    "            best_run_name = run_name\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error: {str(e)}\\n\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nâœ… Best hyperparameters found:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"   {param}: {value}\")\n",
    "\n",
    "best_cv_rmse = best_metrics['rmse']\n",
    "print(f\"\\n   Best RMSE: {best_cv_rmse:.6f}\")\n",
    "\n",
    "# Use the best model from tuning\n",
    "if best_model_name == 'XGBoost':\n",
    "    best_model_tuned = xgb.XGBRegressor(**best_params, tree_method='hist', device='cpu', random_state=RANDOM_STATE)\n",
    "elif best_model_name == 'LightGBM':\n",
    "    best_model_tuned = lgb.LGBMRegressor(**best_params, device='cpu', random_state=RANDOM_STATE, verbose=-1)\n",
    "elif best_model_name == 'Random Forest':\n",
    "    best_model_tuned = RandomForestRegressor(**best_params, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "else:  # Gradient Boosting\n",
    "    best_model_tuned = GradientBoostingRegressor(**best_params, random_state=RANDOM_STATE)\n",
    "\n",
    "best_model_tuned.fit(X_selected, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance & Model Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train best model on full dataset for feature importance\n",
    "# best_model_tuned.fit(X_selected, y)\n",
    "\n",
    "# Get feature importance\n",
    "if hasattr(best_model_tuned, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_selected.columns,\n",
    "        'importance': best_model_tuned.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nðŸ“Š Top 15 Most Important Features:\")\n",
    "    print(feature_importance.head(15).to_string(index=False))\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    top_features = feature_importance.head(15)\n",
    "    top_features.plot(x='feature', y='importance', kind='barh', ax=axes[0], color='steelblue', legend=False)\n",
    "    axes[0].set_title('Top 15 Feature Importances')\n",
    "    axes[0].set_xlabel('Importance')\n",
    "    \n",
    "    # Cumulative importance\n",
    "    cumsum = feature_importance['importance'].cumsum() / feature_importance['importance'].sum()\n",
    "    axes[1].plot(cumsum.values, marker='o', linestyle='-', linewidth=2)\n",
    "    axes[1].axhline(y=0.95, color='r', linestyle='--', label='95% explained variance')\n",
    "    axes[1].set_xlabel('Number of Features')\n",
    "    axes[1].set_ylabel('Cumulative Importance')\n",
    "    axes[1].set_title('Cumulative Feature Importance')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Final evaluation on test split\n",
    "X_train_eval, X_test_eval, y_train_eval, y_test_eval = train_test_split(\n",
    "    X_selected, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "best_model_tuned.fit(X_train_eval, y_train_eval)\n",
    "y_pred_eval = best_model_tuned.predict(X_test_eval)\n",
    "\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test_eval, y_pred_eval))\n",
    "r2_test = r2_score(y_test_eval, y_pred_eval)\n",
    "mae_test = mean_absolute_error(y_test_eval, y_pred_eval)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ“ˆ FINAL MODEL EVALUATION (80/20 Test Split)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"RMSE: {rmse_test:.6f}\")\n",
    "print(f\"RÂ² Score: {r2_test:.6f}\")\n",
    "print(f\"MAE: {mae_test:.6f}\")\n",
    "\n",
    "# Residual analysis\n",
    "residuals = y_test_eval - y_pred_eval\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Residuals vs predictions\n",
    "axes[0, 0].scatter(y_pred_eval, residuals, alpha=0.6)\n",
    "axes[0, 0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Predicted Values')\n",
    "axes[0, 0].set_ylabel('Residuals')\n",
    "axes[0, 0].set_title('Residual Plot')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Histogram of residuals\n",
    "axes[0, 1].hist(residuals, bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Residuals')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Residuals Distribution')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Actual vs Predicted\n",
    "axes[1, 0].scatter(y_test_eval, y_pred_eval, alpha=0.6)\n",
    "axes[1, 0].plot([y_test_eval.min(), y_test_eval.max()], \n",
    "                [y_test_eval.min(), y_test_eval.max()], 'r--', lw=2)\n",
    "axes[1, 0].set_xlabel('Actual Values')\n",
    "axes[1, 0].set_ylabel('Predicted Values')\n",
    "axes[1, 0].set_title(f'Actual vs Predicted (RÂ² = {r2_test:.4f})')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Q-Q plot for residuals\n",
    "from scipy import stats\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Q-Q Plot')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.model.model_signature import infer_signature\n",
    "\n",
    "# Log the final model to Snowflake MLOps\n",
    "\n",
    "# Infer the model signature\n",
    "sig = infer_signature(X_train_eval, y_train_eval)\n",
    "\n",
    "# Log the model using SnowflakeMLOpsManager\n",
    "__snow_mlops.exp.log_model(\n",
    "    model=best_model_tuned,\n",
    "    model_name=f\"{best_model_name}_final\",\n",
    "    signatures={\"predict\": sig},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__snow_mlops.exp.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Predictions for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data with same feature engineering pipeline\n",
    "print(\"ðŸ”„ Preparing test data...\")\n",
    "X_test_raw = engineer_all_features(raw_test, base, df_points, json_points_coord, __geo_tools)\n",
    "X_test_processed = preprocess_features(X_test_raw)\n",
    "\n",
    "# Handle NaN values in test set\n",
    "X_test_processed[numeric_cols] = X_test_processed[numeric_cols].fillna(\n",
    "    X[numeric_cols].median()\n",
    ")\n",
    "\n",
    "# Select same features as training\n",
    "X_test_selected = X_test_processed[selected_features].copy()\n",
    "\n",
    "print(f\"âœ“ Test set shape: {X_test_selected.shape}\")\n",
    "\n",
    "# Ensure test set has same features as training\n",
    "for col in selected_features:\n",
    "    if col not in X_test_selected.columns:\n",
    "        X_test_selected[col] = X[col].median()\n",
    "\n",
    "# Train final model on full training data\n",
    "print(f\"\\nðŸŽ¯ Training final {best_model_name} on full training set...\")\n",
    "best_model_final = best_model_tuned\n",
    "best_model_final.fit(X_selected, y)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred_test = best_model_final.predict(X_test_selected)\n",
    "\n",
    "# Ensure predictions are within valid range (0-1)\n",
    "y_pred_test = np.clip(y_pred_test, 0, 1)\n",
    "\n",
    "print(f\"\\nâœ… Generated predictions for {len(y_pred_test)} test samples\")\n",
    "print(f\"   Min prediction: {y_pred_test.min():.4f}\")\n",
    "print(f\"   Max prediction: {y_pred_test.max():.4f}\")\n",
    "print(f\"   Mean prediction: {y_pred_test.mean():.4f}\")\n",
    "\n",
    "# Create submission file\n",
    "submission = raw_test[['hex_id']].copy()\n",
    "submission['cost_of_living'] = y_pred_test\n",
    "\n",
    "print(f\"\\nðŸ“Š Submission preview:\")\n",
    "print(submission.head(10))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_timestap = pd.Timestamp.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "iteration_name = f\"{best_model_name}_{current_timestap}\"\n",
    "# Save submission\n",
    "submission.to_csv(f'output_{iteration_name}.csv', index=False)\n",
    "print(f\"\\nâœ… Saved predictions to 'output_{iteration_name}.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸŽ‰ MODELING COMPLETE - SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "âœ… Data Processing:\n",
    "   - Loaded training samples: {len(raw_train)}\n",
    "   - Loaded test samples: {len(raw_test)}\n",
    "   - Final feature count: {len(selected_features)}\n",
    "\n",
    "âœ… Feature Engineering:\n",
    "   - Temporal patterns (activity ratios, peak hours)\n",
    "   - Distance-to-POI features\n",
    "   - POI density and composition\n",
    "   - Geographic/regional features\n",
    "\n",
    "âœ… Model Training:\n",
    "   - Models compared: {len(models)}\n",
    "   - Best model: {best_model_name}\n",
    "   - Cross-validation RMSE: {best_cv_rmse:.6f}\n",
    "   - Test RMSE: {rmse_test:.6f}\n",
    "   - Test RÂ²: {r2_test:.6f}\n",
    "\n",
    "ðŸ“ˆ Performance Improvements:\n",
    "   - Previous RMSE baseline: 0.1634\n",
    "   - Current test RMSE: {rmse_test:.6f}\n",
    "   - Improvement: {((0.1634 - rmse_test) / 0.1634 * 100):.2f}%\n",
    "\n",
    "ðŸ“ Output Files:\n",
    "   - Predictions: 'output_final.csv'\n",
    "   - Model: Ready for serialization and deployment\n",
    "\n",
    "ðŸš€ Next Steps:\n",
    "   1. Save model artifacts for production deployment\n",
    "   2. Create API wrapper for real-time predictions\n",
    "   3. Set up monitoring and retraining pipelines\n",
    "   4. Consider ensemble methods if further gains needed\n",
    "   5. Validate on holdout test set\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = pd.read_csv('output.csv')\n",
    "sub1 = pd.read_csv(f'output_{iteration_name}.csv')\n",
    "sub2 = pd.read_csv('output_rf_cv_2.csv')\n",
    "\n",
    "best_weight = 0.33\n",
    "sub1_weight = 0.65\n",
    "sub2_weight = 0.02\n",
    "\n",
    "best['cost_of_living'] = (best_weight * best['cost_of_living']) + (sub1_weight * sub1['cost_of_living']) + (sub2_weight * sub2['cost_of_living']) \n",
    "\n",
    "best.to_csv(f'output_ensemble_{iteration_name}_v3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
